\documentclass[journal]{IEEEtran}


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}
\usepackage{amsmath}
\usepackage{comment}
%\usepackage{cases}
%\usepackage{subeqnarray}

\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.
\title{Bare Demo of IEEEtran.cls for Journals}
%
%
% author names and IEEE memberships
% note positions of commas and nonbreaking spaces ( ~ ) LaTeX will not break
% a structure at a ~ so this keeps an author's name from being broken across
% two lines.
% use \thanks{} to gain access to the first footnote area
% a separate \thanks must be used for each paragraph as LaTeX2e's \thanks
% was not built to handle multiple paragraphs
%

\author{Michael~Shell,~\IEEEmembership{Member,~IEEE,}
        John~Doe,~\IEEEmembership{Fellow,~OSA,}
        and~Jane~Doe,~\IEEEmembership{Life~Fellow,~IEEE}% <-this % stops a space
\thanks{M. Shell is with the Department
of Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta,
GA, 30332 USA e-mail: (see http://www.michaelshell.org/contact.html).}% <-this % stops a space
\thanks{J. Doe and J. Doe are with Anonymous University.}% <-this % stops a space
\thanks{Manuscript received April 19, 2005; revised September 17, 2014.}}

% The paper headers
\markboth{Journal of \LaTeX\ Class Files,~Vol.~13, No.~9, September~2014}%
{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for Journals}
% The only time the second header will appear is for the odd numbered pages
% after the title page when using the twoside option.
%
% *** Note that you probably will NOT want to include the author's ***
% *** name in the headers of peer review papers.                   ***
% You can use \ifCLASSOPTIONpeerreview for conditional compilation here if
% you desire.


% make the title area
\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.
\begin{abstract}
The abstract goes here.
\end{abstract}

% Note that keywords are not normally used for peerreview papers.
\begin{IEEEkeywords}
IEEEtran, journal, \LaTeX, paper, template.
\end{IEEEkeywords}


\IEEEpeerreviewmaketitle



\section{Introduction}
\IEEEPARstart{T}{his}

\section{Subjective IQA}

\section{Objective IQA}

\subsection{FR}

\subsubsection{Spatial Method}
Peak signal-to-noise ($PSNR$) is a classic objective full-reference $IQA$ metric and is commonly used to measure the fidelity of degraded images. $PSNR$ is most easily defined via mean square error ($MSE$). Given a $m \times n$ reference image $I$ and its distorted version $K$, the $MSE$ is defined as equation \ref{eq:PSNR 1}.
\begin{equation}
MSE = \frac{1}{mn} \sum\limits_{i=0}^{m-1} \sum\limits_{j=0}^{n-1} [I(i,j)-K(i,j)]^2
\label{eq:PSNR 1}
\end{equation}
And the $PSNR$ (in $dB$) is defined as:
\begin{equation}
PSNR = 10\log_{10} (\frac{MAX_I^2}{MSE})
\label{eq:PSNR 2}
\end{equation}
where $MAX_I^2$ is the maximum possible pixel value of the $I$ ($i.e.$ peak value). However, in some circumstances, the $PSNR$ is unequal to the perceptual quality prediction task because the human perception is depended on much more complicated factors.



\subsubsection{Transformation-based Method}
Sheikh \textit{et al.} \cite{SheiNSSFR} explored the image quality assessment problem from the perspective of information theory, and they utilized the widely investigated natural scene statistics ($NSS$) models to fit the quality score. Specifically, they adopted two different $NSS$ models to represent the reference image and distorted image. The reference image is expressed by Gaussian Scale Mixtures ($GSM$) in the wavelet domain, while the distorted image is expressed by a simple signal attenuation and additive Gaussian noise model in the wavelet domain. The final information fidelity criterion ($IFC$) is derived from the mutual information between the source and distorted images as equation \ref{eq:NSS 1}-\ref{eq:NSS 2}.
\begin{equation}
I(C^N; D^N | S^N) = \frac{1}{2} \sum\limits_{i=1}^{N}\log_2(1+\frac{g_i^2 s_i^2 \sigma_U^2}{\sigma_V^2})
\label{eq:NSS 1}
\end{equation}

\begin{equation}
IFC = \sum_{k \in subbands}I(C^{N_k,k};D^{N_k,k}|s^{N_k,k})
\label{eq:NSS 2}
\end{equation}
where equation \ref{eq:NSS 1} is the derived for one subband, and the final $IFC$ is the sum over all subbands.



\subsubsection{Learning-based Method}
Narwaria \textit{et al.} \cite{NarwariaFR} developed the machine learning technique and proposed a novel objective $IQA$ metric in 2010. The contribution of this job is using singular value decomposition ($SVD$) as features for quantifying image structural information. And the support vector regression served as a feature fusion tool to generate the quality score. The authors also pointed that the advantages of machine learning technique in $IQA$ devising are that the combination of the features are optimal and the weighting coefficients are adjustable so as to obtain the most appropriate settings.



\subsection{Saliency-Guided IQA}
Human visual attention is the behavioral and cognitive process of selectively concentrating on the special region of the stimulus. Image quality is highly related with artifacts located at salient regions. Saliency-gudied IQA has been widely investigated recently. We divided the saliency-gudied IQA metrics into two categories, which are indirect pooling manner and direct fitting manner. The indirect pooling manner is utilizing human attention information as weighting map to emphasize salient regions so as to promote IQA metrics' performances, while the direct fitting manner aims to explore the relationship between human attention variation and image quality degradation so as to devise new IQA metrics.
Moreover, the indirect pooling manner can be further classified as subjective ground truth-based pooling manner and objective saliency map-based pooling manner.

\subsubsection{Indirect Pooling Manner}
\paragraph{Subjective Ground Truth-based Pooling Manner}

Although objective saliency prediction model has been widely investigated, there is still a gap between state-of-the-art saliency models and real human fixation data. Fortunately, relying on the development of eye-tracking equipment, eye-tracking data has been the most accurate and solid proxy for visual attention information. Eye-tracking data-guided pooling strategy has been explored in recent years for improving IQA metrics.





\section{Conclusion}
The conclusion goes here.






\bibliographystyle{IEEEtran}
\bibliography{Ref}


% that's all folks
\end{document}


